// Code generated by sqlc. DO NOT EDIT.
// versions:
//   sqlc v1.29.0
// source: payload-store.sql

package sqlcv1

import (
	"context"

	"github.com/google/uuid"
	"github.com/jackc/pgx/v5/pgtype"
)

const acquireOrExtendCutoverJobLease = `-- name: AcquireOrExtendCutoverJobLease :one
WITH inputs AS (
    SELECT
        $2::DATE AS key,
        $1::UUID AS lease_process_id,
        $3::TIMESTAMPTZ AS lease_expires_at,
        $4::UUID AS last_tenant_id,
        $5::TIMESTAMPTZ AS last_inserted_at,
        $6::BIGINT AS last_id,
        $7::v1_payload_type AS last_type
), any_lease_held_by_other_process AS (
    -- need coalesce here in case there are no rows that don't belong to this process
    SELECT COALESCE(BOOL_OR(lease_expires_at > NOW()), FALSE) AS lease_exists
    FROM v1_payload_cutover_job_offset
    WHERE lease_process_id != $1::UUID
), to_insert AS (
    SELECT key, lease_process_id, lease_expires_at, last_tenant_id, last_inserted_at, last_id, last_type
    FROM inputs
    -- if a lease is held by another process, we shouldn't try to insert a new row regardless
    -- of which key we're trying to acquire a lease on
    WHERE NOT (SELECT lease_exists FROM any_lease_held_by_other_process)
)

INSERT INTO v1_payload_cutover_job_offset (key, lease_process_id, lease_expires_at, last_tenant_id, last_inserted_at, last_id, last_type)
SELECT ti.key, ti.lease_process_id, ti.lease_expires_at, ti.last_tenant_id, ti.last_inserted_at, ti.last_id, ti.last_type
FROM to_insert ti
ON CONFLICT (key)
DO UPDATE SET
    -- if the lease is held by this process, then we extend the offset to the new tuple of (last_tenant_id, last_inserted_at, last_id, last_type)
    -- otherwise it's a new process acquiring the lease, so we should keep the offset where it was before
    last_tenant_id = CASE
        WHEN EXCLUDED.lease_process_id = v1_payload_cutover_job_offset.lease_process_id THEN EXCLUDED.last_tenant_id
        ELSE v1_payload_cutover_job_offset.last_tenant_id
    END,
    last_inserted_at = CASE
        WHEN EXCLUDED.lease_process_id = v1_payload_cutover_job_offset.lease_process_id THEN EXCLUDED.last_inserted_at
        ELSE v1_payload_cutover_job_offset.last_inserted_at
    END,
    last_id = CASE
        WHEN EXCLUDED.lease_process_id = v1_payload_cutover_job_offset.lease_process_id THEN EXCLUDED.last_id
        ELSE v1_payload_cutover_job_offset.last_id
    END,
    last_type = CASE
        WHEN EXCLUDED.lease_process_id = v1_payload_cutover_job_offset.lease_process_id THEN EXCLUDED.last_type
        ELSE v1_payload_cutover_job_offset.last_type
    END,

    lease_process_id = EXCLUDED.lease_process_id,
    lease_expires_at = EXCLUDED.lease_expires_at
WHERE v1_payload_cutover_job_offset.lease_expires_at < NOW() OR v1_payload_cutover_job_offset.lease_process_id = $1::UUID
RETURNING key, is_completed, lease_process_id, lease_expires_at, last_tenant_id, last_inserted_at, last_id, last_type
`

type AcquireOrExtendCutoverJobLeaseParams struct {
	Leaseprocessid uuid.UUID          `json:"leaseprocessid"`
	Key            pgtype.Date        `json:"key"`
	Leaseexpiresat pgtype.Timestamptz `json:"leaseexpiresat"`
	Lasttenantid   uuid.UUID          `json:"lasttenantid"`
	Lastinsertedat pgtype.Timestamptz `json:"lastinsertedat"`
	Lastid         int64              `json:"lastid"`
	Lasttype       V1PayloadType      `json:"lasttype"`
}

func (q *Queries) AcquireOrExtendCutoverJobLease(ctx context.Context, db DBTX, arg AcquireOrExtendCutoverJobLeaseParams) (*V1PayloadCutoverJobOffset, error) {
	row := db.QueryRow(ctx, acquireOrExtendCutoverJobLease,
		arg.Leaseprocessid,
		arg.Key,
		arg.Leaseexpiresat,
		arg.Lasttenantid,
		arg.Lastinsertedat,
		arg.Lastid,
		arg.Lasttype,
	)
	var i V1PayloadCutoverJobOffset
	err := row.Scan(
		&i.Key,
		&i.IsCompleted,
		&i.LeaseProcessID,
		&i.LeaseExpiresAt,
		&i.LastTenantID,
		&i.LastInsertedAt,
		&i.LastID,
		&i.LastType,
	)
	return &i, err
}

const analyzeV1Payload = `-- name: AnalyzeV1Payload :exec
ANALYZE v1_payload
`

func (q *Queries) AnalyzeV1Payload(ctx context.Context, db DBTX) error {
	_, err := db.Exec(ctx, analyzeV1Payload)
	return err
}

const cleanUpCutoverJobOffsets = `-- name: CleanUpCutoverJobOffsets :exec
DELETE FROM v1_payload_cutover_job_offset
WHERE NOT key = ANY($1::DATE[])
`

func (q *Queries) CleanUpCutoverJobOffsets(ctx context.Context, db DBTX, keystokeep []pgtype.Date) error {
	_, err := db.Exec(ctx, cleanUpCutoverJobOffsets, keystokeep)
	return err
}

const computePayloadBatchSize = `-- name: ComputePayloadBatchSize :one
SELECT compute_payload_batch_size(
    $1::DATE,
    $2::UUID,
    $3::TIMESTAMPTZ,
    $4::BIGINT,
    $5::v1_payload_type,
    $6::INTEGER
) AS total_size_bytes
`

type ComputePayloadBatchSizeParams struct {
	Partitiondate  pgtype.Date        `json:"partitiondate"`
	Lasttenantid   uuid.UUID          `json:"lasttenantid"`
	Lastinsertedat pgtype.Timestamptz `json:"lastinsertedat"`
	Lastid         int64              `json:"lastid"`
	Lasttype       V1PayloadType      `json:"lasttype"`
	Batchsize      int32              `json:"batchsize"`
}

func (q *Queries) ComputePayloadBatchSize(ctx context.Context, db DBTX, arg ComputePayloadBatchSizeParams) (int64, error) {
	row := db.QueryRow(ctx, computePayloadBatchSize,
		arg.Partitiondate,
		arg.Lasttenantid,
		arg.Lastinsertedat,
		arg.Lastid,
		arg.Lasttype,
		arg.Batchsize,
	)
	var total_size_bytes int64
	err := row.Scan(&total_size_bytes)
	return total_size_bytes, err
}

const createPayloadRangeChunks = `-- name: CreatePayloadRangeChunks :many
WITH chunks AS (
    SELECT
        (p).*
    FROM create_payload_offload_range_chunks(
        $1::DATE,
        $2::INTEGER,
        $3::INTEGER,
        $4::UUID,
        $5::TIMESTAMPTZ,
        $6::BIGINT,
        $7::v1_payload_type
    ) p
)

SELECT
    lower_tenant_id::UUID,
    lower_id::BIGINT,
    lower_inserted_at::TIMESTAMPTZ,
    lower_type::v1_payload_type,
    upper_tenant_id::UUID,
    upper_id::BIGINT,
    upper_inserted_at::TIMESTAMPTZ,
    upper_type::v1_payload_type
FROM chunks
`

type CreatePayloadRangeChunksParams struct {
	Partitiondate  pgtype.Date        `json:"partitiondate"`
	Windowsize     int32              `json:"windowsize"`
	Chunksize      int32              `json:"chunksize"`
	Lasttenantid   uuid.UUID          `json:"lasttenantid"`
	Lastinsertedat pgtype.Timestamptz `json:"lastinsertedat"`
	Lastid         int64              `json:"lastid"`
	Lasttype       V1PayloadType      `json:"lasttype"`
}

type CreatePayloadRangeChunksRow struct {
	LowerTenantID   uuid.UUID          `json:"lower_tenant_id"`
	LowerID         int64              `json:"lower_id"`
	LowerInsertedAt pgtype.Timestamptz `json:"lower_inserted_at"`
	LowerType       V1PayloadType      `json:"lower_type"`
	UpperTenantID   uuid.UUID          `json:"upper_tenant_id"`
	UpperID         int64              `json:"upper_id"`
	UpperInsertedAt pgtype.Timestamptz `json:"upper_inserted_at"`
	UpperType       V1PayloadType      `json:"upper_type"`
}

func (q *Queries) CreatePayloadRangeChunks(ctx context.Context, db DBTX, arg CreatePayloadRangeChunksParams) ([]*CreatePayloadRangeChunksRow, error) {
	rows, err := db.Query(ctx, createPayloadRangeChunks,
		arg.Partitiondate,
		arg.Windowsize,
		arg.Chunksize,
		arg.Lasttenantid,
		arg.Lastinsertedat,
		arg.Lastid,
		arg.Lasttype,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*CreatePayloadRangeChunksRow
	for rows.Next() {
		var i CreatePayloadRangeChunksRow
		if err := rows.Scan(
			&i.LowerTenantID,
			&i.LowerID,
			&i.LowerInsertedAt,
			&i.LowerType,
			&i.UpperTenantID,
			&i.UpperID,
			&i.UpperInsertedAt,
			&i.UpperType,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const createV1PayloadCutoverTemporaryTable = `-- name: CreateV1PayloadCutoverTemporaryTable :exec
SELECT copy_v1_payload_partition_structure($1::DATE)
`

func (q *Queries) CreateV1PayloadCutoverTemporaryTable(ctx context.Context, db DBTX, date pgtype.Date) error {
	_, err := db.Exec(ctx, createV1PayloadCutoverTemporaryTable, date)
	return err
}

const diffPayloadSourceAndTargetPartitions = `-- name: DiffPayloadSourceAndTargetPartitions :many
WITH payloads AS (
    SELECT
        (p).*
    FROM diff_payload_source_and_target_partitions($1::DATE) p
)

SELECT
    tenant_id::UUID,
    id::BIGINT,
    inserted_at::TIMESTAMPTZ,
    external_id::UUID,
    type::v1_payload_type,
    location::v1_payload_location,
    COALESCE(external_location_key, '')::TEXT AS external_location_key,
    inline_content::JSONB AS inline_content,
    updated_at::TIMESTAMPTZ
FROM payloads
`

type DiffPayloadSourceAndTargetPartitionsRow struct {
	TenantID            uuid.UUID          `json:"tenant_id"`
	ID                  int64              `json:"id"`
	InsertedAt          pgtype.Timestamptz `json:"inserted_at"`
	ExternalID          uuid.UUID          `json:"external_id"`
	Type                V1PayloadType      `json:"type"`
	Location            V1PayloadLocation  `json:"location"`
	ExternalLocationKey string             `json:"external_location_key"`
	InlineContent       []byte             `json:"inline_content"`
	UpdatedAt           pgtype.Timestamptz `json:"updated_at"`
}

func (q *Queries) DiffPayloadSourceAndTargetPartitions(ctx context.Context, db DBTX, partitiondate pgtype.Date) ([]*DiffPayloadSourceAndTargetPartitionsRow, error) {
	rows, err := db.Query(ctx, diffPayloadSourceAndTargetPartitions, partitiondate)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*DiffPayloadSourceAndTargetPartitionsRow
	for rows.Next() {
		var i DiffPayloadSourceAndTargetPartitionsRow
		if err := rows.Scan(
			&i.TenantID,
			&i.ID,
			&i.InsertedAt,
			&i.ExternalID,
			&i.Type,
			&i.Location,
			&i.ExternalLocationKey,
			&i.InlineContent,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const listPaginatedPayloadsForOffload = `-- name: ListPaginatedPayloadsForOffload :many
WITH payloads AS (
    SELECT
        (p).*
    FROM list_paginated_payloads_for_offload(
        $1::DATE,
        $2::UUID,
        $3::TIMESTAMPTZ,
        $4::BIGINT,
        $5::v1_payload_type,
        $6::UUID,
        $7::TIMESTAMPTZ,
        $8::BIGINT,
        $9::v1_payload_type,
        $10::INTEGER
    ) p
)
SELECT
    tenant_id::UUID,
    id::BIGINT,
    inserted_at::TIMESTAMPTZ,
    external_id::UUID,
    type::v1_payload_type,
    location::v1_payload_location,
    COALESCE(external_location_key, '')::TEXT AS external_location_key,
    inline_content::JSONB AS inline_content,
    updated_at::TIMESTAMPTZ
FROM payloads
`

type ListPaginatedPayloadsForOffloadParams struct {
	Partitiondate  pgtype.Date        `json:"partitiondate"`
	Lasttenantid   uuid.UUID          `json:"lasttenantid"`
	Lastinsertedat pgtype.Timestamptz `json:"lastinsertedat"`
	Lastid         int64              `json:"lastid"`
	Lasttype       V1PayloadType      `json:"lasttype"`
	Nexttenantid   uuid.UUID          `json:"nexttenantid"`
	Nextinsertedat pgtype.Timestamptz `json:"nextinsertedat"`
	Nextid         int64              `json:"nextid"`
	Nexttype       V1PayloadType      `json:"nexttype"`
	Batchsize      int32              `json:"batchsize"`
}

type ListPaginatedPayloadsForOffloadRow struct {
	TenantID            uuid.UUID          `json:"tenant_id"`
	ID                  int64              `json:"id"`
	InsertedAt          pgtype.Timestamptz `json:"inserted_at"`
	ExternalID          uuid.UUID          `json:"external_id"`
	Type                V1PayloadType      `json:"type"`
	Location            V1PayloadLocation  `json:"location"`
	ExternalLocationKey string             `json:"external_location_key"`
	InlineContent       []byte             `json:"inline_content"`
	UpdatedAt           pgtype.Timestamptz `json:"updated_at"`
}

func (q *Queries) ListPaginatedPayloadsForOffload(ctx context.Context, db DBTX, arg ListPaginatedPayloadsForOffloadParams) ([]*ListPaginatedPayloadsForOffloadRow, error) {
	rows, err := db.Query(ctx, listPaginatedPayloadsForOffload,
		arg.Partitiondate,
		arg.Lasttenantid,
		arg.Lastinsertedat,
		arg.Lastid,
		arg.Lasttype,
		arg.Nexttenantid,
		arg.Nextinsertedat,
		arg.Nextid,
		arg.Nexttype,
		arg.Batchsize,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*ListPaginatedPayloadsForOffloadRow
	for rows.Next() {
		var i ListPaginatedPayloadsForOffloadRow
		if err := rows.Scan(
			&i.TenantID,
			&i.ID,
			&i.InsertedAt,
			&i.ExternalID,
			&i.Type,
			&i.Location,
			&i.ExternalLocationKey,
			&i.InlineContent,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const markCutoverJobAsCompleted = `-- name: MarkCutoverJobAsCompleted :exec
UPDATE v1_payload_cutover_job_offset
SET is_completed = TRUE
WHERE key = $1::DATE
`

func (q *Queries) MarkCutoverJobAsCompleted(ctx context.Context, db DBTX, key pgtype.Date) error {
	_, err := db.Exec(ctx, markCutoverJobAsCompleted, key)
	return err
}

const readPayloads = `-- name: ReadPayloads :many
WITH inputs AS (
    SELECT
        UNNEST($1::BIGINT[]) AS id,
        UNNEST($2::TIMESTAMPTZ[]) AS inserted_at,
        UNNEST($3::UUID[]) AS tenant_id,
        UNNEST(CAST($4::TEXT[] AS v1_payload_type[])) AS type
)

SELECT tenant_id, id, inserted_at, external_id, type, location, external_location_key, inline_content, updated_at
FROM v1_payload
WHERE (tenant_id, id, inserted_at, type) IN (
        SELECT tenant_id, id, inserted_at, type
        FROM inputs
    )
`

type ReadPayloadsParams struct {
	Ids         []int64              `json:"ids"`
	Insertedats []pgtype.Timestamptz `json:"insertedats"`
	Tenantids   []uuid.UUID          `json:"tenantids"`
	Types       []string             `json:"types"`
}

func (q *Queries) ReadPayloads(ctx context.Context, db DBTX, arg ReadPayloadsParams) ([]*V1Payload, error) {
	rows, err := db.Query(ctx, readPayloads,
		arg.Ids,
		arg.Insertedats,
		arg.Tenantids,
		arg.Types,
	)
	if err != nil {
		return nil, err
	}
	defer rows.Close()
	var items []*V1Payload
	for rows.Next() {
		var i V1Payload
		if err := rows.Scan(
			&i.TenantID,
			&i.ID,
			&i.InsertedAt,
			&i.ExternalID,
			&i.Type,
			&i.Location,
			&i.ExternalLocationKey,
			&i.InlineContent,
			&i.UpdatedAt,
		); err != nil {
			return nil, err
		}
		items = append(items, &i)
	}
	if err := rows.Err(); err != nil {
		return nil, err
	}
	return items, nil
}

const swapV1PayloadPartitionWithTemp = `-- name: SwapV1PayloadPartitionWithTemp :exec
SELECT swap_v1_payload_partition_with_temp($1::DATE)
`

func (q *Queries) SwapV1PayloadPartitionWithTemp(ctx context.Context, db DBTX, date pgtype.Date) error {
	_, err := db.Exec(ctx, swapV1PayloadPartitionWithTemp, date)
	return err
}

const writePayloads = `-- name: WritePayloads :exec
WITH inputs AS (
    SELECT DISTINCT
        UNNEST($1::BIGINT[]) AS id,
        UNNEST($2::TIMESTAMPTZ[]) AS inserted_at,
        UNNEST($3::UUID[]) AS external_id,
        UNNEST(CAST($4::TEXT[] AS v1_payload_type[])) AS type,
        UNNEST(CAST($5::TEXT[] AS v1_payload_location[])) AS location,
        UNNEST($6::TEXT[]) AS external_location_key,
        UNNEST($7::JSONB[]) AS inline_content,
        UNNEST($8::UUID[]) AS tenant_id
)

INSERT INTO v1_payload (
    tenant_id,
    id,
    inserted_at,
    external_id,
    type,
    location,
    external_location_key,
    inline_content
)
SELECT
    i.tenant_id,
    i.id,
    i.inserted_at,
    i.external_id,
    i.type,
    i.location,
    CASE WHEN i.external_location_key = '' OR i.location != 'EXTERNAL' THEN NULL ELSE i.external_location_key END,
    i.inline_content
FROM
    inputs i
ORDER BY i.tenant_id, i.inserted_at, i.id, i.type
ON CONFLICT (tenant_id, id, inserted_at, type)
DO UPDATE SET
    location = EXCLUDED.location,
    external_location_key = CASE WHEN EXCLUDED.external_location_key = '' OR EXCLUDED.location != 'EXTERNAL' THEN NULL ELSE EXCLUDED.external_location_key END,
    inline_content = EXCLUDED.inline_content,
    updated_at = NOW()
`

type WritePayloadsParams struct {
	Ids                  []int64              `json:"ids"`
	Insertedats          []pgtype.Timestamptz `json:"insertedats"`
	Externalids          []uuid.UUID          `json:"externalids"`
	Types                []string             `json:"types"`
	Locations            []string             `json:"locations"`
	Externallocationkeys []string             `json:"externallocationkeys"`
	Inlinecontents       [][]byte             `json:"inlinecontents"`
	Tenantids            []uuid.UUID          `json:"tenantids"`
}

func (q *Queries) WritePayloads(ctx context.Context, db DBTX, arg WritePayloadsParams) error {
	_, err := db.Exec(ctx, writePayloads,
		arg.Ids,
		arg.Insertedats,
		arg.Externalids,
		arg.Types,
		arg.Locations,
		arg.Externallocationkeys,
		arg.Inlinecontents,
		arg.Tenantids,
	)
	return err
}
