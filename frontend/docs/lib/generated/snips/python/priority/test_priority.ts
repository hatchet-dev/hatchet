import { Snippet } from '@/lib/generated/snips/types';

const snippet: Snippet = {
  'language': 'python',
  'content': 'import asyncio\nfrom datetime import datetime, timedelta\nfrom random import choice\nfrom subprocess import Popen\nfrom typing import Any, AsyncGenerator, Literal\nfrom uuid import uuid4\n\nimport pytest\nimport pytest_asyncio\nfrom pydantic import BaseModel\n\nfrom examples.priority.worker import DEFAULT_PRIORITY, SLEEP_TIME, priority_workflow\nfrom hatchet_sdk import Hatchet, ScheduleTriggerWorkflowOptions, TriggerWorkflowOptions\nfrom hatchet_sdk.clients.rest.models.v1_task_status import V1TaskStatus\n\nPriority = Literal[\'low\', \'medium\', \'high\', \'default\']\n\n\nclass RunPriorityStartedAt(BaseModel):\n    priority: Priority\n    started_at: datetime\n    finished_at: datetime\n\n\ndef priority_to_int(priority: Priority) -> int:\n    match priority:\n        case \'high\':\n            return 3\n        case \'medium\':\n            return 2\n        case \'low\':\n            return 1\n        case \'default\':\n            return DEFAULT_PRIORITY\n        case _:\n            raise ValueError(f\'Invalid priority: {priority}\')\n\n\n@pytest_asyncio.fixture(loop_scope=\'session\', scope=\'function\')\nasync def dummy_runs() -> None:\n    priority: Priority = \'high\'\n\n    await priority_workflow.aio_run_many_no_wait(\n        [\n            priority_workflow.create_bulk_run_item(\n                options=TriggerWorkflowOptions(\n                    priority=(priority_to_int(priority)),\n                    additional_metadata={\n                        \'priority\': priority,\n                        \'key\': ix,\n                        \'type\': \'dummy\',\n                    },\n                )\n            )\n            for ix in range(40)\n        ]\n    )\n\n    await asyncio.sleep(3)\n\n    return None\n\n\n@pytest.mark.parametrize(\n    \'on_demand_worker\',\n    [\n        (\n            [\'poetry\', \'run\', \'python\', \'examples/priority/worker.py\', \'--slots\', \'1\'],\n            8003,\n        )\n    ],\n    indirect=True,\n)\n@pytest.mark.asyncio(loop_scope=\'session\')\nasync def test_priority(\n    hatchet: Hatchet, dummy_runs: None, on_demand_worker: Popen[Any]\n) -> None:\n    test_run_id = str(uuid4())\n    choices: list[Priority] = [\'low\', \'medium\', \'high\', \'default\']\n    N = 30\n\n    run_refs = await priority_workflow.aio_run_many_no_wait(\n        [\n            priority_workflow.create_bulk_run_item(\n                options=TriggerWorkflowOptions(\n                    priority=(priority_to_int(priority := choice(choices))),\n                    additional_metadata={\n                        \'priority\': priority,\n                        \'key\': ix,\n                        \'test_run_id\': test_run_id,\n                    },\n                )\n            )\n            for ix in range(N)\n        ]\n    )\n\n    await asyncio.gather(*[r.aio_result() for r in run_refs])\n\n    workflows = (\n        await hatchet.workflows.aio_list(workflow_name=priority_workflow.name)\n    ).rows\n\n    assert workflows\n\n    workflow = next((w for w in workflows if w.name == priority_workflow.name), None)\n\n    assert workflow\n\n    assert workflow.name == priority_workflow.name\n\n    runs = await hatchet.runs.aio_list(\n        workflow_ids=[workflow.metadata.id],\n        additional_metadata={\n            \'test_run_id\': test_run_id,\n        },\n        limit=1_000,\n    )\n\n    runs_ids_started_ats: list[RunPriorityStartedAt] = sorted(\n        [\n            RunPriorityStartedAt(\n                priority=(r.additional_metadata or {}).get(\'priority\') or \'low\',\n                started_at=r.started_at or datetime.min,\n                finished_at=r.finished_at or datetime.min,\n            )\n            for r in runs.rows\n        ],\n        key=lambda x: x.started_at,\n    )\n\n    assert len(runs_ids_started_ats) == len(run_refs)\n    assert len(runs_ids_started_ats) == N\n\n    for i in range(len(runs_ids_started_ats) - 1):\n        curr = runs_ids_started_ats[i]\n        nxt = runs_ids_started_ats[i + 1]\n\n        \'\'\'Run start times should be in order of priority\'\'\'\n        assert priority_to_int(curr.priority) >= priority_to_int(nxt.priority)\n\n        \'\'\'Runs should proceed one at a time\'\'\'\n        assert curr.finished_at <= nxt.finished_at\n        assert nxt.finished_at >= nxt.started_at\n\n        \'\'\'Runs should finish after starting (this is mostly a test for engine datetime handling bugs)\'\'\'\n        assert curr.finished_at >= curr.started_at\n\n\n@pytest.mark.parametrize(\n    \'on_demand_worker\',\n    [\n        (\n            [\'poetry\', \'run\', \'python\', \'examples/priority/worker.py\', \'--slots\', \'1\'],\n            8003,\n        )\n    ],\n    indirect=True,\n)\n@pytest.mark.asyncio(loop_scope=\'session\')\nasync def test_priority_via_scheduling(\n    hatchet: Hatchet, dummy_runs: None, on_demand_worker: Popen[Any]\n) -> None:\n    test_run_id = str(uuid4())\n    sleep_time = 3\n    n = 30\n    choices: list[Priority] = [\'low\', \'medium\', \'high\', \'default\']\n    run_at = datetime.now() + timedelta(seconds=sleep_time)\n\n    versions = await asyncio.gather(\n        *[\n            priority_workflow.aio_schedule(\n                run_at=run_at,\n                options=ScheduleTriggerWorkflowOptions(\n                    priority=(priority_to_int(priority := choice(choices))),\n                    additional_metadata={\n                        \'priority\': priority,\n                        \'key\': ix,\n                        \'test_run_id\': test_run_id,\n                    },\n                ),\n            )\n            for ix in range(n)\n        ]\n    )\n\n    await asyncio.sleep(sleep_time * 2)\n\n    workflow_id = versions[0].workflow_id\n\n    attempts = 0\n\n    while True:\n        if attempts >= SLEEP_TIME * n * 2:\n            raise TimeoutError(\'Timed out waiting for runs to finish\')\n\n        attempts += 1\n        await asyncio.sleep(1)\n        runs = await hatchet.runs.aio_list(\n            workflow_ids=[workflow_id],\n            additional_metadata={\n                \'test_run_id\': test_run_id,\n            },\n            limit=1_000,\n        )\n\n        if not runs.rows:\n            continue\n\n        if any(\n            r.status in [V1TaskStatus.FAILED, V1TaskStatus.CANCELLED] for r in runs.rows\n        ):\n            raise ValueError(\'One or more runs failed or were cancelled\')\n\n        if all(r.status == V1TaskStatus.COMPLETED for r in runs.rows):\n            break\n\n    runs_ids_started_ats: list[RunPriorityStartedAt] = sorted(\n        [\n            RunPriorityStartedAt(\n                priority=(r.additional_metadata or {}).get(\'priority\') or \'low\',\n                started_at=r.started_at or datetime.min,\n                finished_at=r.finished_at or datetime.min,\n            )\n            for r in runs.rows\n        ],\n        key=lambda x: x.started_at,\n    )\n\n    assert len(runs_ids_started_ats) == len(versions)\n\n    for i in range(len(runs_ids_started_ats) - 1):\n        curr = runs_ids_started_ats[i]\n        nxt = runs_ids_started_ats[i + 1]\n\n        \'\'\'Run start times should be in order of priority\'\'\'\n        assert priority_to_int(curr.priority) >= priority_to_int(nxt.priority)\n\n        \'\'\'Runs should proceed one at a time\'\'\'\n        assert curr.finished_at <= nxt.finished_at\n        assert nxt.finished_at >= nxt.started_at\n\n        \'\'\'Runs should finish after starting (this is mostly a test for engine datetime handling bugs)\'\'\'\n        assert curr.finished_at >= curr.started_at\n\n\n@pytest_asyncio.fixture(loop_scope=\'session\', scope=\'function\')\nasync def crons(\n    hatchet: Hatchet, dummy_runs: None\n) -> AsyncGenerator[tuple[str, str, int], None]:\n    test_run_id = str(uuid4())\n    choices: list[Priority] = [\'low\', \'medium\', \'high\']\n    n = 30\n\n    crons = await asyncio.gather(\n        *[\n            hatchet.cron.aio_create(\n                workflow_name=priority_workflow.name,\n                cron_name=f\'{test_run_id}-cron-{i}\',\n                expression=\'* * * * *\',\n                input={},\n                additional_metadata={\n                    \'trigger\': \'cron\',\n                    \'test_run_id\': test_run_id,\n                    \'priority\': (priority := choice(choices)),\n                    \'key\': str(i),\n                },\n                priority=(priority_to_int(priority)),\n            )\n            for i in range(n)\n        ]\n    )\n\n    yield crons[0].workflow_id, test_run_id, n\n\n    await asyncio.gather(*[hatchet.cron.aio_delete(cron.metadata.id) for cron in crons])\n\n\ndef time_until_next_minute() -> float:\n    now = datetime.now()\n    next_minute = (now + timedelta(minutes=1)).replace(second=0, microsecond=0)\n\n    return (next_minute - now).total_seconds()\n\n\n@pytest.mark.parametrize(\n    \'on_demand_worker\',\n    [\n        (\n            [\'poetry\', \'run\', \'python\', \'examples/priority/worker.py\', \'--slots\', \'1\'],\n            8003,\n        )\n    ],\n    indirect=True,\n)\n@pytest.mark.asyncio(loop_scope=\'session\')\nasync def test_priority_via_cron(\n    hatchet: Hatchet, crons: tuple[str, str, int], on_demand_worker: Popen[Any]\n) -> None:\n    workflow_id, test_run_id, n = crons\n\n    await asyncio.sleep(time_until_next_minute() + 10)\n\n    attempts = 0\n\n    while True:\n        if attempts >= SLEEP_TIME * n * 2:\n            raise TimeoutError(\'Timed out waiting for runs to finish\')\n\n        attempts += 1\n        await asyncio.sleep(1)\n        runs = await hatchet.runs.aio_list(\n            workflow_ids=[workflow_id],\n            additional_metadata={\n                \'test_run_id\': test_run_id,\n            },\n            limit=1_000,\n        )\n\n        if not runs.rows:\n            continue\n\n        if any(\n            r.status in [V1TaskStatus.FAILED, V1TaskStatus.CANCELLED] for r in runs.rows\n        ):\n            raise ValueError(\'One or more runs failed or were cancelled\')\n\n        if all(r.status == V1TaskStatus.COMPLETED for r in runs.rows):\n            break\n\n    runs_ids_started_ats: list[RunPriorityStartedAt] = sorted(\n        [\n            RunPriorityStartedAt(\n                priority=(r.additional_metadata or {}).get(\'priority\') or \'low\',\n                started_at=r.started_at or datetime.min,\n                finished_at=r.finished_at or datetime.min,\n            )\n            for r in runs.rows\n        ],\n        key=lambda x: x.started_at,\n    )\n\n    assert len(runs_ids_started_ats) == n\n\n    for i in range(len(runs_ids_started_ats) - 1):\n        curr = runs_ids_started_ats[i]\n        nxt = runs_ids_started_ats[i + 1]\n\n        \'\'\'Run start times should be in order of priority\'\'\'\n        assert priority_to_int(curr.priority) >= priority_to_int(nxt.priority)\n\n        \'\'\'Runs should proceed one at a time\'\'\'\n        assert curr.finished_at <= nxt.finished_at\n        assert nxt.finished_at >= nxt.started_at\n\n        \'\'\'Runs should finish after starting (this is mostly a test for engine datetime handling bugs)\'\'\'\n        assert curr.finished_at >= curr.started_at\n',
  'source': 'out/python/priority/test_priority.py',
  'blocks': {},
  'highlights': {}
};  // Then replace double quotes with single quotes

export default snippet;
