import { Callout, Cards, Steps } from "nextra/components";
import BatchProcessingDiagram from "@/components/BatchProcessingDiagramWrapper";

# Batch Processing

Batch processing involves running the same operation across a large set of items, such as images, documents, records, or API calls. This page covers how to structure batch workloads as Hatchet workflows with fan-out, retry, and concurrency control.

<BatchProcessingDiagram />

## Core Challenges

<Steps>

### Parallelism

A parent task fans out to one child per item. Hatchet distributes these across available workers. Adding more workers increases throughput without code changes.

### Partial failure

Each item is an independent task. If one fails, Hatchet retries just that item; the rest continue. You can also bulk-retry all failed items from the dashboard.

### Resource control

Concurrency limits prevent overwhelming your infrastructure. Rate limits protect external APIs. Priority queues let urgent batches run ahead of lower-priority work.

</Steps>

## Key Features

| Feature                                                  | What it does for batch processing                                       |
| -------------------------------------------------------- | ----------------------------------------------------------------------- |
| **[Child Spawning](/features/child-spawning)**               | Fan out to one task per item with automatic distribution across workers |
| **[Bulk Run](/guide/bulk-run)**                              | Trigger thousands of tasks in a single API call                         |
| **[Retry Policies](/features/retry-policies)**               | Retry failed items individually without restarting the batch            |
| **[Bulk Retries](/features/bulk-retries-and-cancellations)** | Re-run all failed items from the dashboard                              |
| **[Concurrency](/features/concurrency)**                     | Limit how many items process simultaneously                             |
| **[Rate Limits](/features/rate-limits)**                     | Throttle external API calls across all workers                          |
| **[Priority](/features/priority)**                           | Urgent batches jump ahead of lower-priority work                        |
| **[Autoscaling](/guide/autoscaling-workers)**                | Scale workers up during batch processing, down when idle                |

<Callout type="info">
  The batch processing pattern is [Fanout](/guide/patterns/fanout) applied at
  scale. For fixed multi-stage processing (e.g., validate → transform → load),
  combine with [Pre-Determined
  Pipelines](/guide/patterns/pre-determined-pipelines).
</Callout>

## Architecture

<Steps>

### Trigger the batch

Start a parent workflow with the batch input (a list of item IDs, file paths, or records). This can come from an API call, event, cron schedule, or the dashboard.

### Fan out to workers

The parent task iterates over the input and spawns one child task per item. For very large batches (10,000+ items), use `BulkRunChild` for optimized dispatching.

### Process items

Each child task processes its item independently, calling external APIs, transforming data, and writing results. Failed items are retried according to your retry policy.

### Collect results

The parent awaits all children and aggregates results. You can see the status of every item in real-time in the Hatchet dashboard.

</Steps>

<Callout type="warning">
  For batches with thousands of items, use **durable workflows** so the parent
  task doesn't hold a worker slot while waiting for all children to complete.
  See [Durable Workflows](/guide/durable-workflows-overview) for details.
</Callout>

## Common Batch Patterns

| Pattern                   | Description                                                                         |
| ------------------------- | ----------------------------------------------------------------------------------- |
| **Image processing**      | Resize, transcode, or analyze images in parallel across workers                     |
| **Data enrichment**       | Enrich records by calling external APIs (geocoding, company info, email validation) |
| **Report generation**     | Generate per-customer reports in parallel, then aggregate into a summary            |
| **Database migrations**   | Process and migrate records in batches with retry and progress tracking             |
| **Notification delivery** | Send emails, SMS, or push notifications to a user list with rate limiting           |

## Related Patterns

<Cards>
  <Cards.Card title="Fanout" href="/guide/patterns/fanout">
    The core pattern behind batch processing, spawning N children from a parent.
  </Cards.Card>
  <Cards.Card
    title="Pre-Determined Pipelines"
    href="/guide/patterns/pre-determined-pipelines"
  >
    Chain batch processing with multi-stage transforms in a DAG.
  </Cards.Card>
  <Cards.Card title="RAG & Indexing" href="/guide/use-cases/rag-and-indexing">
    A specialized batch processing use case for document indexing pipelines.
  </Cards.Card>
  <Cards.Card title="Cycles" href="/guide/patterns/cycles">
    Process paginated results one page at a time with iterative child spawning.
  </Cards.Card>
</Cards>

## Next Steps

- [Child Spawning](/features/child-spawning): learn the fan-out API for batch processing
- [Bulk Run](/guide/bulk-run): trigger large batches efficiently
- [Concurrency Control](/features/concurrency): limit concurrent item processing
- [Rate Limits](/features/rate-limits): protect external APIs during batch operations
