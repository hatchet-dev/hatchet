import { Callout, Card, Cards, Steps, Tabs } from "nextra/components";
import UniversalTabs from "@/components/UniversalTabs";
import { GithubSnippet, getSnippets } from "@/components/code";

export const SimpleTs = {
  path: "src/v1/examples/simple/worker.ts",
};

export const getStaticProps = ({}) => getSnippets([SimpleTs]);

# Workers

Workers are the backbone of Hatchet, responsible for executing the individual steps defined within your workflows. They operate autonomously across different nodes in your infrastructure, allowing for distributed and scalable task execution. Understanding how to deploy and manage workers effectively is crucial to fully leverage the power of Hatchet.


## How Workers Operate

In Hatchet, workers are long-running processes that wait for instructions from the Hatchet engine to execute specific steps. They communicate with the Hatchet engine to receive tasks, execute them, and report back the results.

Here are the key characteristics of workers in Hatchet:

1. **Distributed Execution**: Workers can be deployed across multiple systems or even different cloud environments, enabling distributed task execution.

2. **Language Agnostic**: Workers can be implemented in various programming languages, as long as they can communicate with the Hatchet engine and execute the required steps.

3. **Scalability**: By adding more workers, you can scale your system horizontally to handle increased loads and distribute tasks across a wider set of resources.

## Declaring a Worker

Now that we have a [hatchet client](./introduction.mdx) and a [workflow declaration](./basic-workflows.mdx) we can declare a worker that can execute the workflow tasks.

Declare a worker by calling the `worker` method on the hatchet client. The `worker` method takes a name and an optional configuration object.

<UniversalTabs items={["Python", "Typescript", "Go"]}>
  <Tabs.Tab title="Python">
    ```python
    def main() -> None:
      worker = hatchet.worker("test-worker", slots=1, workflows=[simple])
      worker.start()

    if __name__ == "__main__":
        main()
    ```

  </Tabs.Tab>
  <Tabs.Tab title="Typescript">
    ### Register the Worker
    <GithubSnippet src={SimpleTs} target="Declaring a Worker" />

    ### Add an Entrypoint Script

    Add a script to your `package.json` to start the worker (changing the file path to the location of your worker file):

    ```json
    "scripts": {
      "start:worker": "ts-node src/v1/examples/simple/worker.ts"
    }
    ```

    ### Run the Worker

    Start the worker by running the script you just added to your `package.json`:
    <UniversalTabs items={["npm", "pnpm", "yarn"]} optionKey="packageManager">
      <Tabs.Tab title="npm">
        ```bash
        npm run start:worker
        ```
      </Tabs.Tab>
      <Tabs.Tab title="pnpm">
        ```bash
        pnpm run start:worker
        ```
      </Tabs.Tab>
      <Tabs.Tab title="yarn">
        ```bash
        yarn start:worker
        ```
      </Tabs.Tab>
    </UniversalTabs>

  </Tabs.Tab>
  <Tabs.Tab title="Go">
    ```go
  	w, err := worker.NewWorker(
      worker.WithClient(
        c,
      ),
    )

    cleanup, err := w.Start()
    ```

  </Tabs.Tab>
</UniversalTabs>

## Understanding Slots

Slots are the number of concurrent _task_ runs that a worker can execute. For instance, if you set `slots=5` on your worker, then your worker will be able to run five tasks concurrently before new tasks start needing to wait in the queue before being picked up.

As a simple example, if we have a worker with `slots` set to `1` and we have a single task declared that sleeps for 1 second, then if we submit more than one task per second, the tasks will begin to queue up, as the worker can only handle one task at a time. Increasing the number of `slots` on your worker will allow you to handle more concurrent work (and thus more throughput, in many cases), without needing to run more workers.

An important caveat is that slot-level concurrency is only helpful up to the point where the worker is not bottlenecked by another resource, such as CPU, memory, or network bandwidth. If your worker is bottlenecked by one of these resources, increasing the number of slots will not improve throughput.

## Best Practices for Managing Workers

To ensure a robust and efficient Hatchet implementation, consider the following best practices when managing your workers:

1. **Reliability**: Deploy workers in a stable environment with sufficient resources to avoid resource contention and ensure reliable execution.

2. **Monitoring and Logging**: Implement robust monitoring and logging mechanisms to track worker health, performance, and task execution status.

3. **Error Handling**: Design workers to handle errors gracefully, report execution failures to Hatchet, and retry tasks based on configured policies.

4. **Secure Communication**: Ensure secure communication between workers and the Hatchet engine, especially when distributed across different networks.

5. **Lifecycle Management**: Implement proper lifecycle management for workers, including automatic restarts on critical failures and graceful shutdown procedures.

6. **Scalability**: Plan for scalability by designing your system to easily add or remove workers based on demand, leveraging containerization, orchestration tools, or cloud auto-scaling features.

7. **Consistent Updates**: Keep worker implementations up to date with the latest Hatchet SDKs and ensure compatibility with the Hatchet engine version.
