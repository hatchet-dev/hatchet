import { Callout } from "nextra/components";

# Hatchet Managed Compute

## Overview

Hatchet Managed Compute provides the simplicity of serverless while delivering the performance and control of traditional infrastructure, making it ideal for long lived, or data intensive AI applications and background job processing. It enables dynamic scaling while eliminating common serverless limitations like cold starts and timeouts.

### High-Availability Computing

- **Sub-100ms Instance Provisioning**: Pre-warms instances before resource demands
- **Distributed Architecture**: Built on [Hatchet Queue](../home/) for reliable workload distribution
- **[Multi-Region Support](./compute/cpu.mdx#available-regions)**: Deploy across regions for fault tolerance and data locality
- **[Auto-Scaling](./compute/auto-scaling.mdx)**: Automatically scale your compute resources based on workload demand

### Available Compute Classes

- [Shared CPUs](./compute/cpu.mdx#shared-cpu)
- [Performance CPUs](./compute/cpu.mdx#performance-cpu)
- [GPU instances](./compute/gpu.mdx)

### Smart Workload Management

- **State-Aware**: Routes tasks to instances with preloaded models/resources using [worker labels](../worker-affinity.mdx)
- **Burstable Capacity**: Scales dynamically based on queue depth
- **Sticky Assignment**: Routes tasks to the same instance when possible using [sticky assignments](../sticky-assignment.mdx)

## Deployment

- **[GitOps Integration](./compute/git-ops.mdx)**: Automatic builds and deployments on commit
- **Zero-Ops**: Managed infrastructure eliminates operational overhead
- **Version Control**: Infrastructure changes tracked in code

## Advantages Over Serverless

1. No cold starts or execution timeouts
2. Predictable performance
3. Cost-effective for sustained workloads
4. Fine-grained control over compute resources
5. Better suited for AI and data processing tasks
