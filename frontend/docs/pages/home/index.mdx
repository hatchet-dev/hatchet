import { Tabs, Callout } from "nextra/components";

# What is Hatchet?

Hatchet is a modern orchestration platform for background tasks that helps engineering teams build ultra low-latency and high throughput data ingestion and agentic ai pipelines.

You write simple functions in Python, Typescript, and Go and run them on [workers](./workers.mdx) in your own infrastructure. You can compose these functions into [workflows as code](./child-spawning.mdx) or as [Directed Acyclic Graphs (DAGs)](./dags.mdx) to build complex data pipelines.

Hatchet handles scheduling, complex assignment, fault tolerance, and observability so your workload scales and you can focus on building your application.

## Why Hatchet?

Hatchet may be a good fit if you're experiencing or expecting complex data or compute workflows in your application where speed and parallelism are critical. Hatchet's performance characteristics are fast enough for real-time workloads without sacrificing reliability or cost.

âš¡ï¸ **Low-Latency For Real-Time Workloads** - Sub-35ms task dispatch for hot workers with thousands of concurrent tasks. Smart assignment rules handle [rate-limits](./rate-limits.mdx), [fairness](./concurrency.mdx), and [priorities](./priority.mdx) without complex configuration.

ðŸª¨ **Durability for Long Running Jobs** - Every task invocation is durably logged to PostgreSQL. With [durable execution](./durable-execution.mdx), when jobs fail your workflow will resume exactly where you left off â€” no lost work, no duplicate LLM calls, no engineer headaches.

ðŸ§˜â€â™‚ï¸ **Zen Developer Experience** - Hatchet SDKs (Python, Typescript, and Go) are hand-built with modern tooling and are designed to be easy to use. Hatchet has built-in observability and debugging tools for things like replays, logs, and alerts.

If you plan on self-hosting or have requirements for on-premise deployment, there are some additional considerations:

ðŸ˜ **Minimal Infra Dependencies** - Hatchet is built on top of PostgreSQL and for simple workloads, [its all you need](./self-hosting/hatchet-lite.mdx).

â¬†ï¸ **Fully Featured Open Source** - Hatchet is fully open source, so you can run the same application code against [Hatchet Cloud](https://cloud.onhatchet.run) to get started quickly or [self-host](./self-hosting.mdx) when you need more control.

## Hatchet vs. Alternatives

Today, developers who need background task processing and workflow orchestration face two main options:

1. Adopt external services like Temporal or Airflow, which are powerful but complex to run or introduce latency, or
2. Use simple task queue libraries like Celery or BullMQ, which lack critical workflow features and become difficult to debug at scale.

| Metric                  | Hatchet     | Celery   | Airflow | Temporal                   |
| ----------------------- | ----------- | -------- | ------- | -------------------------- |
| **Task Start Latency**  | ~100ms      | ~100ms+  | 5-30s   | ~100ms+                    |
| **Concurrent Tasks**    | Thousands   | Limited  | Limited | Expensive                  |
| **Durability**          | PostgreSQL  | Redis/DB | DB      | Cassandra/PostgreSQL/MySQL |
| **Checkpoint Recovery** | âœ… Built-in | âŒ       | âŒ      | âœ… Complex                 |
| **Real-time Streaming** | âœ… Native   | âŒ       | âŒ      | Limited                    |

## Core Concepts

**Background tasks**

Background tasks are functions which are executed outside of the main request/response cycle of your application. Traditionally, they are invoked from your application code, from an external event (like a webhook), or on a schedule (like a cron job). Background tasks are useful for offloading work from your application, and for running complex, long-running or resource-intensive tasks.

Recently, background tasks have become more important as AI agents and LLMs have become more prevalent -- moving expensive, long running, or error-prone workload off of your main API and onto dedicated infrastructure.

**Workers**

Hatchet is responsible for invoking tasks which run on **workers**. Workers are long-running processes which are connected to Hatchet, and execute the functions defined in your tasks. They can be run on your own infrastructure on services like Kubernetes, AWS ECS, or managed PaaS like [Porter](https://porter.run), [Ryvn](https://ryvn.ai), [Fly.io](https://fly.io), [Railway](https://railway.com), or [Render](https://render.com).

One of the design goals of Hatchet is to ensure that workers can be run anywhere, from a PaaS like Heroku to a Kubernetes cluster running in your own data center. We refer to workers which are connected with an available slot as **hot workers**.

**What is a task?**

A task is a unit of work that can be executed by Hatchet. Tasks can be run directly, or can be executed in response to an external trigger (an event, schedule, or API call). For example, if you'd like to send notifications to a user after they've signed up, you could create a task for that. Tasks can be [spawned from within another task](./child-spawning.mdx) or can be built into a [directed acyclic graph based workflow](./dags.mdx).

**Durable queue**

Hatchet is built on top of a durable, low-latency queue, which means it can handle real-time interactions and business-critical tasks. This is particularly useful if you're building a real-time application, or if you're running tasks which need to be completed quickly. It can scale to millions of queued tasks and can handle thousands of tasks per second. We are continuously working to improve our throughput and latency.

## Production Readiness

Hatchet has been battle-tested in production environments since 2023, processing billions of tasks per month for scale-ups and enterprises across various industries. Financial services companies rely on Hatchet for payment processing workflows where reliability is non-negotiable. Health insurance companies use it for critical claims processing workflows. SaaS applications leverage Hatchet to manage everything from user onboarding sequences to billing workflows and notification campaigns.

> "With Hatchet, we've scaled our indexing workflows effortlessly, reducing failed runs by 50% and doubling our user base in just two weeks!"
> â€” Soohoon, Co-Founder @ Greptile

> "Hatchet enables Aevy to process up to 50,000 documents in under an hour through optimized parallel execution, compared to nearly a week with our previous setup."
> â€” Ymir, CTO @ Aevy

AI companies have found Hatchet particularly valuable for orchestrating real-time AI agents, model training pipelines, and massive document processing workflows. Data teams use it for ETL workflows that need sophisticated error handling and dependency management.

## Quick Starts

We have a number of quick start tutorials for getting up and running quickly with Hatchet:

- [Hatchet Cloud Quickstart](./hatchet-cloud-quickstart.mdx)
- [Hatchet Self-Hosted Quickstarts](./self-hosting.mdx)

We also have a number of guides for getting started with the Hatchet SDKs:

- [Python SDK Quickstart](https://github.com/hatchet-dev/hatchet-python-quickstart)
- [Typescript SDK Quickstart](https://github.com/hatchet-dev/hatchet-typescript-quickstart)
- [Go SDK Quickstart](https://github.com/hatchet-dev/hatchet-go-quickstart)

## Learn More

Ready to dive deeper? Explore these additional resources:

**[Architecture](./architecture.mdx)** - Learn how Hatchet is built and designed for scale.

**[Guarantees & Tradeoffs](./guarantees-and-tradeoffs.mdx)** - Understand Hatchet's guarantees, limitations, and when to use it.

Or get started with the **[Hatchet Cloud Quickstart](./hatchet-cloud-quickstart.mdx)** or **[self-hosting](./self-hosting.mdx)**.
