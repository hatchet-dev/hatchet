import { Callout } from "nextra/components";

# Batch Assignment (Beta)

<Callout type="info">
  Batch assignment is currently in beta. Expect rapid iteration on the data
  model, APIs, and UI while we gather feedback.
</Callout>

Batch assignment lets Hatchet buffer individual task invocations and flush them
as configurable batches for a worker to process together. This is ideal when
workloads benefit from shared setup (e.g. vectorized work with shared memory,
batched external API calls, or upstream rate limits that prefer grouped
delivery). Hatchet batches per **step** and **batch key**, then assigns a flushed
batch to a single worker.

## Configuring a batched task

To enable batching for a step, set its **batch configuration** (via workflow
definitions/manifests or SDK helpers). The current fields are:

- `batch_size` (required): the maximum number of items to flush at once. Must be
  at least `1`.
- `flush_interval_ms` (optional): the maximum time (in milliseconds) an item can
  wait before a flush is triggered. **Defaults to `1000`.**
- `batch_key` (optional): a CEL expression that groups tasks into logical
  batches. Tasks with different keys are always processed separately. **Defaults
  to `'default'`.**
- `max_runs` (optional): the maximum number of concurrently running batches per
  key. Additional batches wait for an earlier batch with the same key to finish.
  **Defaults to `100`.**

`batch_size` controls throughput; `flush_interval_ms` bounds latency for
sporadic traffic; `batch_key` controls partitioning; `max_runs` bounds batch
concurrency per key.

## Supplying a batch key

The batch key expression runs with the same CEL context used by other Hatchet
features. It can reference `input`, `additionalMetadata`, `workflowRunId`, and
parent trigger data. The expression must evaluate to a **string**; Hatchet
trims whitespace. An empty/whitespace result falls back to the default key
(`'default'`).

```cel
// group by tenant and logical bucket
input.customerId + ":" + string(additionalMetadata.bucket)
```

If the expression fails to evaluate or produces an unsupported type, Hatchet
rejects task creation to keep buffering logic predictable. Use CEL tooling
locally to validate expressions before deploying.

## Worker execution model

Workers that opt into batchable tasks receive an array of inputs instead of a
single payload. SDKs expose this through dedicated helpers (for example,
`hatchet.batchTask` in the TypeScript SDK) that pass the aggregated inputs (and
per-item contexts) to your handler. Items continue to respect per-task retry
policies; batching changes how tasks are *delivered*, not how they’re retried.

```ts
const batch = hatchet.batchTask({
  name: "simple",
  batchSize: 2,
  flushInterval: 1_000, // defaults to 1000ms if omitted
  batchKey: "input.batchId", // defaults to `'default'` if omitted
  maxRuns: 100, // defaults to 100 if omitted
  fn: async (inputs, ctxs) => {
    return inputs.map((input, index) => ({
      message: `${input.Message.toLowerCase()}#${index}`,
    }));
  },
});
```

When a batch flushes, Hatchet records `batchId`, `batchSize`, `batchIndex`, and
`batchKey` on each task’s runtime metadata. This is available via the API (and
SDK contexts) so you can trace which invocations ran together.

## Observability and lifecycle

Hatchet emits dedicated events while tasks wait for a batch (`WAITING_FOR_BATCH`
and `BATCH_BUFFERED`) and once a flush is triggered (`BATCH_FLUSHED`). These
events surface in the run timeline on the dashboard. `BATCH_FLUSHED` includes a
reason (for example `batch_size_reached` or `interval_elapsed`).

Batch runs are reference counted inside the repository. When all tasks in a
batch complete (successfully or by cancellation), Hatchet automatically releases
the batch reservation, allowing queued tasks with the same key to start building
the next batch.

## Notes on evaluation order

Concurrency, rate limiting, sticky assignment, manual slot releases, and other
advanced features continue to work alongside batching. Hatchet evaluates task
configuration first; when tasks are eligible to be scheduled, they are buffered
and flushed per **batch key**.
