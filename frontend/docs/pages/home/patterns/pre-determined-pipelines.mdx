import { Callout, Cards, Steps } from "nextra/components";
import PipelineDiagram from "@/components/PipelineDiagram";
import PatternComparison from "@/components/PatternComparison";

# Pre-Determined Pipelines

A **pre-determined pipeline** is a workflow where the sequence of tasks and their dependencies are defined ahead of time as a Directed Acyclic Graph (DAG). Unlike [fanout](/home/patterns/fanout), where children are spawned dynamically at runtime, pipelines have a fixed structure that is known before execution begins.

<PipelineDiagram />

## How It Works

<Steps>

### Declare a workflow

Define a named workflow that acts as the container for your pipeline. This is the entry point for running the entire pipeline.

### Define tasks with dependencies

Each task in the pipeline specifies its parent tasks. Hatchet uses these dependencies to build a DAG and determines the execution order automatically. Tasks without dependencies run first; tasks with parents wait until all parents complete.

### Pass data between tasks

Tasks can read the outputs of their parent tasks through the context object. This lets you thread data through the pipeline without external state.

</Steps>

<Callout type="info">
  Pre-determined pipelines are built using [DAG Workflows](/home/dags). See that
  page for full code examples on defining workflows, adding tasks with
  dependencies, accessing parent outputs, and running workflows.
</Callout>

## When to Use Pipelines vs Fanout

|                  | Pre-Determined Pipelines                           | Fanout                                    |
| ---------------- | -------------------------------------------------- | ----------------------------------------- |
| **Structure**    | Fixed at definition time                           | Dynamic at runtime                        |
| **Task count**   | Known ahead of time                                | Determined by input data                  |
| **Dependencies** | Explicit parent-child DAG                          | Parent spawns N identical children        |
| **Best for**     | ETL, data processing stages, multi-step transforms | Batch processing, parallel map operations |

## In Workflows vs Durable Workflows

<PatternComparison
  rows={[
    { label: "Structure", workflow: "Tasks and dependencies declared upfront as a DAG", durable: "A single durable function with sequential checkpoint calls" },
    { label: "Parallelism", workflow: "Independent tasks run in parallel automatically", durable: "Parallelism requires spawning child tasks with RunChild" },
    { label: "Intermediate results", workflow: "Cached between tasks — each task starts fresh on retry", durable: "Stored in the durable event log — replayed on restart" },
    { label: "Best for", workflow: "Most pipelines — ETL, multi-step transforms, CI/CD", durable: "Pipelines that need inline SleepFor or WaitForEvent between steps" },
  ]}
  recommendation="workflow"
  recommendationText="Pre-determined pipelines are the strongest fit for regular DAG workflows. The fixed structure maps directly to a DAG with automatic parallelism and cached results. Only reach for durable workflows if your pipeline needs long waits between steps."
/>

## Use Cases

<Cards>
  <Cards.Card title="ETL Pipelines" href="/home/dags">
    Extract data, transform it through multiple stages, and load it into a
    destination — each stage as a task with clear dependencies.
  </Cards.Card>
  <Cards.Card title="CI/CD Workflows" href="/home/dags">
    Build, test, and deploy in sequence with parallel test suites that fan back
    into a deploy step.
  </Cards.Card>
  <Cards.Card title="Document Processing" href="/home/dags">
    Parse, validate, enrich, and index documents through a fixed sequence of
    processing stages.
  </Cards.Card>
  <Cards.Card title="Multi-Step AI Pipelines" href="/home/dags">
    Chain LLM calls with validation gates — generate, evaluate, refine — where
    each step depends on the previous.
  </Cards.Card>
</Cards>

## Next Steps

- [DAG Workflows](/home/dags) — full guide to defining workflows with task dependencies
- [Fanout](/home/patterns/fanout) — dynamically spawn tasks at runtime instead
- [Concurrency Control](/home/concurrency) — limit concurrent task execution within a pipeline
- [Procedural Child Spawning](/home/child-spawning) — combine pipelines with dynamic child tasks
