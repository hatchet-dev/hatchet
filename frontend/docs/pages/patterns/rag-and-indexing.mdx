import { Callout, Cards, Steps } from "nextra/components";
import RAGPipelineDiagram from "@/components/RAGPipelineDiagramWrapper";

# RAG & Data Indexing

RAG and indexing pipelines share a common shape: ingest documents, split them into chunks, generate embeddings, and write to a vector database. This page covers how to structure these pipelines as Hatchet workflows.

<RAGPipelineDiagram />

## Core Challenges

<Steps>

### Fan-out to many documents

When new data arrives, a parent task fans out to child tasks, one per document or chunk. Hatchet distributes these across your workers. The number of children is determined at runtime by the input data.

### Rate-limiting embedding APIs

Embedding providers (OpenAI, Cohere, Voyage) enforce rate limits. Hatchet's rate limiting throttles embedding tasks across all workers to stay within provider quotas.

### Retrying without re-processing

If a task fails (embedding API timeout, database write error), Hatchet retries just that task, not the entire pipeline. Completed chunks stay indexed.

</Steps>

## Key Features

| Feature                                                                                  | What it does for RAG                                                           |
| ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------ |
| **[Child Spawning](/concepts/durable-workflows/directed-acyclic-graphs/child-spawning)** | Fan out to one task per document/chunk; process thousands in parallel          |
| **[Rate Limits](/concepts/rate-limits)**                                                 | Throttle embedding API calls across all workers to stay within provider limits |
| **[Retry Policies](/concepts/retry-policies)**                                           | Retry failed chunks individually without re-processing the whole pipeline      |
| **[Concurrency](/concepts/concurrency)**                                                 | Fair scheduling with GROUP_ROUND_ROBIN for multi-tenant indexing               |
| **[DAG Workflows](/concepts/durable-workflows/directed-acyclic-graphs)**                 | Chain pipeline stages (ingest → chunk → embed → index) with clear dependencies |
| **[Event Triggers](/concepts/run-on-event)**                                             | Start indexing automatically when new documents arrive                         |
| **[Cron Triggers](/concepts/cron-runs)**                                                 | Schedule periodic re-indexing or incremental updates                           |
| **[Bulk Run](/concepts/bulk-run)**                                                       | Trigger indexing for thousands of documents in a single API call               |

<Callout type="info">
  RAG pipelines are a natural fit for [Pre-Determined
  Pipelines](/patterns/pre-determined-pipelines). The stages (ingest, chunk,
  embed, index) are fixed and map directly to a DAG. Use
  [Fanout](/patterns/fanout) within the chunking stage to parallelize.
</Callout>

## Pipeline Architecture

<Steps>

### Ingest

A trigger (event, cron, or API call) starts the pipeline with document references. The ingest task fetches raw content from your data source (S3, database, API).

### Chunk

The parent task fans out to child tasks, one per document. Each child splits its document into chunks using your preferred strategy (fixed-size, semantic, recursive).

### Embed

Each chunk task calls your embedding provider. Hatchet rate-limits these calls across all workers to respect API quotas. Failed embeddings are retried individually.

### Index

Embeddings are written to your vector database (Pinecone, Weaviate, pgvector). Hatchet retries on transient database errors without re-computing embeddings.

### Query (optional)

A separate workflow handles user queries, generating query embeddings, searching the vector database, and passing context to the LLM.

</Steps>

<Callout type="warning">
  When fanning out to many chunks, ensure your workers have enough slots or use
  [Concurrency Control](/concepts/concurrency) to limit how many run
  simultaneously.
</Callout>

## Multi-Tenant Indexing

For SaaS applications where multiple tenants share the same pipeline:

- **GROUP_ROUND_ROBIN concurrency** distributes scheduling fairly so no single tenant monopolizes workers
- **Additional metadata** tags each run with a tenant ID for filtering in the dashboard
- **Priority queues** allow higher-priority indexing jobs to run ahead of lower-priority ones

## Related Patterns

<Cards>
  <Cards.Card
    title="Pre-Determined Pipelines"
    href="/patterns/pre-determined-pipelines"
  >
    The fixed-stage DAG pattern that RAG pipelines are built on.
  </Cards.Card>
  <Cards.Card title="Fanout" href="/patterns/fanout">
    Parallelize document and chunk processing across your worker fleet.
  </Cards.Card>
  <Cards.Card title="Cycles" href="/patterns/cycles">
    Implement incremental indexing that re-crawls until all changes are
    processed.
  </Cards.Card>
  <Cards.Card title="Batch Processing" href="/patterns/batch-processing">
    General-purpose batch processing patterns that apply to indexing workloads.
  </Cards.Card>
</Cards>

## Next Steps

- [DAG Workflows](/concepts/durable-workflows/directed-acyclic-graphs): define multi-stage pipelines with task dependencies
- [Rate Limits](/concepts/rate-limits): configure rate limiting for embedding APIs
- [Child Spawning](/concepts/durable-workflows/directed-acyclic-graphs/child-spawning): fan out to per-document tasks
- [Concurrency Control](/concepts/concurrency): fair scheduling for multi-tenant indexing
