---
asIndexPage: true
---
import { Callout } from "nextra/components";

# What is Hatchet?

Hatchet is a modern orchestration platform that helps engineering teams build low-latency and high-throughput data ingestion and agentic AI pipelines.

You write functions in Python, Typescript, Go, or Ruby and let Hatchet handle scheduling, retries, fault tolerance, and observability. The core mental model has three parts:

- **[Tasks](/essentials/your-first-task)** â€” the fundamental unit of work. A task wraps a single function and gives Hatchet everything it needs to schedule, execute, and observe it.
- **[Workers](/essentials/workers)** â€” long-running processes in your infrastructure that pick up and execute tasks.
- **[Durable Workflows](/essentials/durable-workflows)** â€” compose multiple tasks into durable pipelines with dependencies, retries, and checkpointing.

All tasks and workflows are **defined as code**, making them easy to version, test, and deploy.

## Use cases

While Hatchet is a general-purpose orchestration platform, it's particularly well-suited for:

- **Real-time data processing** â€” data ingestion for keeping LLM contexts up-to-date, ETL pipelines that require fast execution and high throughput.
- **AI agents** â€” features like [webhooks](/concepts/webhooks), [child spawning](/concepts/child-spawning), and dynamic workflows are designed to support agentic patterns.
- **Event-driven systems** â€” Hatchet's [eventing features](/concepts/run-on-event) let you build event-driven architectures without additional infrastructure.

## Why Hatchet?

âš¡ï¸ **Low-Latency For Real-Time Workloads** - Sub-25ms task dispatch for hot workers with thousands of concurrent tasks. Smart assignment rules handle [rate-limits](/concepts/rate-limits), [fairness](/concepts/concurrency), and [priorities](/concepts/priority) without complex configuration.

ðŸª¨ **Durability for Long Running Jobs** - Every task invocation is durably logged to PostgreSQL. With [durable execution](/concepts/durable-execution), when jobs fail your workflow will resume exactly where you left off â€” no lost work, no duplicate LLM calls, no engineer headaches.

ðŸ§˜â€â™‚ï¸ **Zen Developer Experience** - Hatchet SDKs (Python, Typescript, and Go) are built with modern tooling and are designed to be easy to use. Hatchet has built-in observability and debugging tools for things like replays, logs, and alerts.

If you plan on self-hosting or have requirements for an on-premise deployment, there are some additional considerations:

ðŸ˜ **Minimal Infra Dependencies** - Hatchet is built on top of PostgreSQL and for simple workloads, [its all you need](/self-hosting/hatchet-lite).

â¬†ï¸ **Fully Featured Open Source** - Hatchet is 100% MIT licensed, so you can run the same application code against [Hatchet Cloud](https://cloud.onhatchet.run) to get started quickly or [self-host](/self-hosting) when you need more control.

## Hatchet vs. Alternatives

Today, developers who need background task processing and workflow orchestration face two main options:

1. Adopt external services like Temporal or Airflow, which are powerful but complex to run or introduce latency, or
2. Use simple task queue libraries like Celery or BullMQ, which lack critical workflow features and become difficult to debug at scale.

| Feature                                     | Hatchet | Celery     | Airflow    | Temporal     |
| ------------------------------------------- | ------- | ---------- | ---------- | ------------ |
| **Task Start Latency**                      | 25ms    | 5-100ms+   | 5-30s      | 25ms         |
| **Concurrent Tasks**                        | 1000s   | Variable\* | Variable\* | 10000        |
| **Code-First Workflows**                    | âœ…      | âœ…         | âœ…         | âœ…           |
| **Cron Jobs and Scheduling**                | âœ…      | âœ…         | âœ…         | âœ…           |
| **Priority Queues**                         | âœ…      | âœ…         | âœ…         | âœ… (beta)    |
| **Durable Sleep/Checkpoints**               | âœ…      | âŒ         | âŒ         | âœ…           |
| **Sticky Assignment/Complex Routing Logic** | âœ…      | âŒ         | âŒ         | âœ… (limited) |
| **Event-Based Triggering**                  | âœ…      | âŒ         | âœ…         | âŒ           |
| **Real-time Streaming**                     | âœ…      | âŒ         | âŒ         | âŒ           |
| **Global Rate Limits**                      | âœ…      | âŒ         | âŒ         | âŒ           |
| **Event Streaming**                         | âœ…      | âŒ         | âŒ         | âŒ           |

\*Requires careful configuration and infrastructure scaling

## Production Readiness

Hatchet has been battle-tested in production environments, processing billions of tasks per month for scale-ups and enterprises across various industries. Our open source offering is deployed over 10k times per month, while Hatchet Cloud supports hundreds of companies running at scale.

> "With Hatchet, we've scaled our indexing workflows effortlessly, reducing failed runs by 50% and doubling our user base in just two weeks!"
> â€” Soohoon, Co-Founder @ Greptile

> "Hatchet enables Aevy to process up to 50,000 documents in under an hour through optimized parallel execution, compared to nearly a week with our previous setup."
> â€” Ymir, CTO @ Aevy

## Ready to dive deeper?

Check out the **[Architecture & Guarantees](/essentials/architecture-and-guarantees)** page to learn how Hatchet is built, its guarantees, and when to use it.

Or get started with the **[Hatchet Cloud Quickstart](/essentials/quickstart)** or **[self-hosting](/self-hosting)**.
