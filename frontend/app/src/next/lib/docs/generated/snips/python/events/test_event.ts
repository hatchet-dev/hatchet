import { Snippet } from '@/next/lib/docs/generated/snips/types';

const snippet: Snippet = {
  language: 'python',
  content:
    'import asyncio\nimport json\nfrom contextlib import asynccontextmanager\nfrom datetime import datetime, timedelta, timezone\nfrom typing import AsyncGenerator, cast\nfrom uuid import uuid4\n\nimport pytest\nfrom pydantic import BaseModel\n\nfrom examples.events.worker import (\n    EVENT_KEY,\n    SECONDARY_KEY,\n    WILDCARD_KEY,\n    EventWorkflowInput,\n    event_workflow,\n)\nfrom hatchet_sdk.clients.events import (\n    BulkPushEventOptions,\n    BulkPushEventWithMetadata,\n    PushEventOptions,\n)\nfrom hatchet_sdk.clients.rest.models.v1_task_status import V1TaskStatus\nfrom hatchet_sdk.clients.rest.models.v1_task_summary import V1TaskSummary\nfrom hatchet_sdk.contracts.events_pb2 import Event\nfrom hatchet_sdk.hatchet import Hatchet\n\n\nclass ProcessedEvent(BaseModel):\n    id: str\n    payload: dict[str, str | bool]\n    meta: dict[str, str | bool | int]\n    should_have_runs: bool\n    test_run_id: str\n\n    def __hash__(self) -> int:\n        return hash(self.model_dump_json())\n\n\n@asynccontextmanager\nasync def event_filter(\n    hatchet: Hatchet,\n    test_run_id: str,\n    expression: str | None = None,\n    payload: dict[str, str] = {},\n) -> AsyncGenerator[None, None]:\n    expression = (\n        expression\n        or f"input.should_skip == false && payload.test_run_id == \'{test_run_id}\'"\n    )\n\n    f = await hatchet.filters.aio_create(\n        workflow_id=event_workflow.id,\n        expression=expression,\n        scope=test_run_id,\n        payload={"test_run_id": test_run_id, **payload},\n    )\n\n    try:\n        yield\n    finally:\n        await hatchet.filters.aio_delete(f.metadata.id)\n\n\nasync def fetch_runs_for_event(\n    hatchet: Hatchet, event: Event\n) -> tuple[ProcessedEvent, list[V1TaskSummary]]:\n    runs = await hatchet.runs.aio_list(triggering_event_external_id=event.eventId)\n\n    meta = (\n        cast(dict[str, str | int | bool], json.loads(event.additionalMetadata))\n        if event.additionalMetadata\n        else {}\n    )\n    payload = (\n        cast(dict[str, str | bool], json.loads(event.payload)) if event.payload else {}\n    )\n\n    processed_event = ProcessedEvent(\n        id=event.eventId,\n        payload=payload,\n        meta=meta,\n        should_have_runs=meta.get("should_have_runs", False) is True,\n        test_run_id=cast(str, meta["test_run_id"]),\n    )\n\n    if not all([r.output for r in runs.rows]):\n        return (processed_event, [])\n\n    return (\n        processed_event,\n        runs.rows or [],\n    )\n\n\nasync def wait_for_result(\n    hatchet: Hatchet, events: list[Event]\n) -> dict[ProcessedEvent, list[V1TaskSummary]]:\n    await asyncio.sleep(3)\n\n    since = datetime.now(tz=timezone.utc) - timedelta(minutes=2)\n\n    persisted = (await hatchet.event.aio_list(limit=100, since=since)).rows or []\n\n    assert {e.eventId for e in events}.issubset({e.metadata.id for e in persisted})\n\n    iters = 0\n    while True:\n        print("Waiting for event runs to complete...")\n        if iters > 15:\n            print("Timed out waiting for event runs to complete.")\n            return {\n                ProcessedEvent(\n                    id=event.eventId,\n                    payload=json.loads(event.payload) if event.payload else {},\n                    meta=(\n                        json.loads(event.additionalMetadata)\n                        if event.additionalMetadata\n                        else {}\n                    ),\n                    should_have_runs=False,\n                    test_run_id=cast(\n                        str, json.loads(event.additionalMetadata).get("test_run_id", "")\n                    ),\n                ): []\n                for event in events\n            }\n\n        iters += 1\n\n        event_runs = await asyncio.gather(\n            *[fetch_runs_for_event(hatchet, event) for event in events]\n        )\n\n        all_empty = all(not event_run for _, event_run in event_runs)\n\n        if all_empty:\n            await asyncio.sleep(1)\n            continue\n\n        event_id_to_runs = {event_id: runs for (event_id, runs) in event_runs}\n\n        any_queued_or_running = any(\n            run.status in [V1TaskStatus.QUEUED, V1TaskStatus.RUNNING]\n            for runs in event_id_to_runs.values()\n            for run in runs\n        )\n\n        if any_queued_or_running:\n            await asyncio.sleep(1)\n            continue\n\n        break\n\n    return event_id_to_runs\n\n\nasync def wait_for_result_and_assert(hatchet: Hatchet, events: list[Event]) -> None:\n    event_to_runs = await wait_for_result(hatchet, events)\n\n    for event, runs in event_to_runs.items():\n        await assert_event_runs_processed(event, runs)\n\n\nasync def assert_event_runs_processed(\n    event: ProcessedEvent,\n    runs: list[V1TaskSummary],\n) -> None:\n    runs = [\n        run\n        for run in runs\n        if (run.additional_metadata or {}).get("hatchet__event_id") == event.id\n    ]\n\n    if event.should_have_runs:\n        assert len(runs) > 0\n\n        for run in runs:\n            assert run.status == V1TaskStatus.COMPLETED\n            assert run.output.get("test_run_id") == event.test_run_id\n    else:\n        assert len(runs) == 0\n\n\ndef bpi(\n    index: int = 1,\n    test_run_id: str = "",\n    should_skip: bool = False,\n    should_have_runs: bool = True,\n    key: str = EVENT_KEY,\n    payload: dict[str, str] = {},\n    scope: str | None = None,\n) -> BulkPushEventWithMetadata:\n    return BulkPushEventWithMetadata(\n        key=key,\n        payload={\n            "should_skip": should_skip,\n            **payload,\n        },\n        additional_metadata={\n            "should_have_runs": should_have_runs,\n            "test_run_id": test_run_id,\n            "key": index,\n        },\n        scope=scope,\n    )\n\n\ndef cp(should_skip: bool) -> dict[str, bool]:\n    return EventWorkflowInput(should_skip=should_skip).model_dump()\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_push(hatchet: Hatchet) -> None:\n    e = hatchet.event.push(EVENT_KEY, cp(False))\n\n    assert e.eventId is not None\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_async_event_push(hatchet: Hatchet) -> None:\n    e = await hatchet.event.aio_push(EVENT_KEY, cp(False))\n\n    assert e.eventId is not None\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_async_event_bulk_push(hatchet: Hatchet) -> None:\n    events = [\n        BulkPushEventWithMetadata(\n            key="event1",\n            payload={"message": "This is event 1", "should_skip": False},\n            additional_metadata={"source": "test", "user_id": "user123"},\n        ),\n        BulkPushEventWithMetadata(\n            key="event2",\n            payload={"message": "This is event 2", "should_skip": False},\n            additional_metadata={"source": "test", "user_id": "user456"},\n        ),\n        BulkPushEventWithMetadata(\n            key="event3",\n            payload={"message": "This is event 3", "should_skip": False},\n            additional_metadata={"source": "test", "user_id": "user789"},\n        ),\n    ]\n    opts = BulkPushEventOptions(namespace="bulk-test")\n\n    e = await hatchet.event.aio_bulk_push(events, opts)\n\n    assert len(e) == 3\n\n    # Sort both lists of events by their key to ensure comparison order\n    sorted_events = sorted(events, key=lambda x: x.key)\n    sorted_returned_events = sorted(e, key=lambda x: x.key)\n    namespace = "bulk-test"\n\n    # Check that the returned events match the original events\n    for original_event, returned_event in zip(sorted_events, sorted_returned_events):\n        assert returned_event.key == namespace + original_event.key\n\n\n@pytest.fixture(scope="function")\ndef test_run_id() -> str:\n    return str(uuid4())\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_engine_behavior(hatchet: Hatchet) -> None:\n    test_run_id = str(uuid4())\n    events = [\n        bpi(\n            test_run_id=test_run_id,\n        ),\n        bpi(\n            test_run_id=test_run_id,\n            key="thisisafakeeventfoobarbaz",\n            should_have_runs=False,\n        ),\n    ]\n\n    result = await hatchet.event.aio_bulk_push(events)\n\n    await wait_for_result_and_assert(hatchet, result)\n\n\ndef gen_bulk_events(test_run_id: str) -> list[BulkPushEventWithMetadata]:\n    return [\n        ## No scope, so it shouldn\'t have any runs\n        bpi(\n            index=1,\n            test_run_id=test_run_id,\n            should_skip=False,\n            should_have_runs=False,\n        ),\n        ## No scope, so it shouldn\'t have any runs\n        bpi(\n            index=2,\n            test_run_id=test_run_id,\n            should_skip=True,\n            should_have_runs=False,\n        ),\n        ## Scope is set and `should_skip` is False, so it should have runs\n        bpi(\n            index=3,\n            test_run_id=test_run_id,\n            should_skip=False,\n            should_have_runs=True,\n            scope=test_run_id,\n        ),\n        ## Scope is set and `should_skip` is True, so it shouldn\'t have runs\n        bpi(\n            index=4,\n            test_run_id=test_run_id,\n            should_skip=True,\n            should_have_runs=False,\n            scope=test_run_id,\n        ),\n        ## Scope is set, `should_skip` is False, but key is different, so it shouldn\'t have runs\n        bpi(\n            index=5,\n            test_run_id=test_run_id,\n            should_skip=True,\n            should_have_runs=False,\n            scope=test_run_id,\n            key="thisisafakeeventfoobarbaz",\n        ),\n        ## Scope is set, `should_skip` is False, but key is different, so it shouldn\'t have runs\n        bpi(\n            index=6,\n            test_run_id=test_run_id,\n            should_skip=False,\n            should_have_runs=False,\n            scope=test_run_id,\n            key="thisisafakeeventfoobarbaz",\n        ),\n    ]\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_skipping_filtering(hatchet: Hatchet, test_run_id: str) -> None:\n    async with event_filter(hatchet, test_run_id):\n        events = gen_bulk_events(test_run_id)\n\n        result = await hatchet.event.aio_bulk_push(events)\n\n        await wait_for_result_and_assert(hatchet, result)\n\n\nasync def bulk_to_single(hatchet: Hatchet, event: BulkPushEventWithMetadata) -> Event:\n    return await hatchet.event.aio_push(\n        event_key=event.key,\n        payload=event.payload,\n        options=PushEventOptions(\n            scope=event.scope,\n            additional_metadata=event.additional_metadata,\n            priority=event.priority,\n        ),\n    )\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_skipping_filtering_no_bulk(\n    hatchet: Hatchet, test_run_id: str\n) -> None:\n    async with event_filter(hatchet, test_run_id):\n        raw_events = gen_bulk_events(test_run_id)\n        events = await asyncio.gather(\n            *[bulk_to_single(hatchet, event) for event in raw_events]\n        )\n\n        await wait_for_result_and_assert(hatchet, events)\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_payload_filtering(hatchet: Hatchet, test_run_id: str) -> None:\n    async with event_filter(\n        hatchet,\n        test_run_id,\n        "input.should_skip == false && payload.foobar == \'baz\'",\n        {"foobar": "qux"},\n    ):\n        event = await hatchet.event.aio_push(\n            event_key=EVENT_KEY,\n            payload={"message": "This is event 1", "should_skip": False},\n            options=PushEventOptions(\n                scope=test_run_id,\n                additional_metadata={\n                    "should_have_runs": False,\n                    "test_run_id": test_run_id,\n                    "key": 1,\n                },\n            ),\n        )\n\n        await wait_for_result_and_assert(hatchet, [event])\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_event_payload_filtering_with_payload_match(\n    hatchet: Hatchet, test_run_id: str\n) -> None:\n    async with event_filter(\n        hatchet,\n        test_run_id,\n        "input.should_skip == false && payload.foobar == \'baz\'",\n        {"foobar": "baz"},\n    ):\n        event = await hatchet.event.aio_push(\n            event_key=EVENT_KEY,\n            payload={"message": "This is event 1", "should_skip": False},\n            options=PushEventOptions(\n                scope=test_run_id,\n                additional_metadata={\n                    "should_have_runs": True,\n                    "test_run_id": test_run_id,\n                    "key": 1,\n                },\n            ),\n        )\n\n        await wait_for_result_and_assert(hatchet, [event])\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_filtering_by_event_key(hatchet: Hatchet, test_run_id: str) -> None:\n    async with event_filter(\n        hatchet,\n        test_run_id,\n        f"event_key == \'{SECONDARY_KEY}\'",\n    ):\n        event_1 = await hatchet.event.aio_push(\n            event_key=SECONDARY_KEY,\n            payload={\n                "message": "Should run because filter matches",\n                "should_skip": False,\n            },\n            options=PushEventOptions(\n                scope=test_run_id,\n                additional_metadata={\n                    "should_have_runs": True,\n                    "test_run_id": test_run_id,\n                },\n            ),\n        )\n        event_2 = await hatchet.event.aio_push(\n            event_key=EVENT_KEY,\n            payload={\n                "message": "Should skip because filter does not match",\n                "should_skip": False,\n            },\n            options=PushEventOptions(\n                scope=test_run_id,\n                additional_metadata={\n                    "should_have_runs": False,\n                    "test_run_id": test_run_id,\n                },\n            ),\n        )\n\n        await wait_for_result_and_assert(hatchet, [event_1, event_2])\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_key_wildcards(hatchet: Hatchet, test_run_id: str) -> None:\n    keys = [\n        WILDCARD_KEY.replace("*", "1"),\n        WILDCARD_KEY.replace("*", "2"),\n        "foobar",\n        EVENT_KEY,\n    ]\n\n    async with event_filter(\n        hatchet,\n        test_run_id,\n    ):\n        events = [\n            await hatchet.event.aio_push(\n                event_key=key,\n                payload={\n                    "should_skip": False,\n                },\n                options=PushEventOptions(\n                    scope=test_run_id,\n                    additional_metadata={\n                        "should_have_runs": key != "foobar",\n                        "test_run_id": test_run_id,\n                    },\n                ),\n            )\n            for key in keys\n        ]\n\n        await wait_for_result_and_assert(hatchet, events)\n\n\n@pytest.mark.asyncio(loop_scope="session")\nasync def test_multiple_runs_for_multiple_scope_matches(\n    hatchet: Hatchet, test_run_id: str\n) -> None:\n    async with event_filter(\n        hatchet, test_run_id, payload={"filter_id": "1"}, expression="1 == 1"\n    ):\n        async with event_filter(\n            hatchet, test_run_id, payload={"filter_id": "2"}, expression="2 == 2"\n        ):\n            event = await hatchet.event.aio_push(\n                event_key=EVENT_KEY,\n                payload={\n                    "should_skip": False,\n                },\n                options=PushEventOptions(\n                    scope=test_run_id,\n                    additional_metadata={\n                        "should_have_runs": True,\n                        "test_run_id": test_run_id,\n                    },\n                ),\n            )\n\n            event_to_runs = await wait_for_result(hatchet, [event])\n\n            assert len(event_to_runs.keys()) == 1\n\n            runs = list(event_to_runs.values())[0]\n\n            assert len(runs) == 2\n\n            assert {r.output.get("filter_id") for r in runs} == {"1", "2"}\n',
  source: 'out/python/events/test_event.py',
  blocks: {},
  highlights: {},
};

export default snippet;
